---
- set_fact:
    selected_host: "{{ groups['all'] | map('extract', hostvars) | selectattr('system_roles', 'defined') | selectattr('system_roles', 'contains', 'control_plane') | map(attribute='inventory_hostname') | random }}"
    control_plane: "{{ groups['all'] | map('extract', hostvars) | selectattr('system_roles', 'defined') | selectattr('system_roles', 'contains', 'control_plane') | map(attribute='inventory_hostname') }}"
    worker: "{{ groups['all'] | map('extract', hostvars) | selectattr('system_roles', 'defined') | selectattr('system_roles', 'contains', 'worker') | map(attribute='inventory_hostname') }}"
    clusterDNS: >-
      {%- set base_ip = serviceSubnet.split('/')[0] -%}
      {%- set octets = base_ip.split('.') -%}
      {{ octets[0] }}.{{ octets[1] }}.{{ octets[2] }}.{{ octets[3]|int + 10 }}
  run_once: true

- name: system configurations
  include_tasks: system.yml
  when: inventory_hostname in worker or inventory_hostname in control_plane

- name: force-reinstall
  shell: /etc/init.d/kubelet stop; kubeadm reset --force; rm -rf kubeadm.log /etc/kubernetes/pki/ /etc/kubernetes/manifests/ /etc/kubernetes/tmp/ /etc/kubernetes/*.conf  /var/log/kubelet/kubelet.log /var/log/crio/crio.log /var/lib/kubelet/*; iptables -F; iptables -t nat -F; /etc/init.d/crio restart; ip link delete dev cni0; ip link delete dev flannel.1; rm  /opt/cni/bin/flannel /etc/cni/net.d/10-flannel.conflist; /etc/init.d/kubelet restart
  ignore_errors: yes
  when: "'force_reinstall' in ansible_run_tags" 
  tags:
    - never
    - force_reinstall

- name: check kubernetes status
  import_tasks: k8s-status.yaml
  failed_when: true

- name: prepare images
  import_tasks: prepare_images.yml

- name: install cluster if needed
  include_tasks: init.yml
  when: not k8s_init_done and inventory_hostname == selected_host

- name: kubelet configuration
  template:
    src: kubelet-config.yaml
    dest: "{{ kubelet_config }}"

- name: get etcd image
  #shell:  kubectl get pod -n kube-system etcd-{{ healthy_node }}  -o yaml -o jsonpath='{.spec.containers[?(@.name=="etcd")].image}'
  shell:  "kubeadm config images list --kubernetes-version {{ current_k8s_version if current_k8s_version else k8s_version }} --image-repository {{ registry }}/{{ kubernetes_image_section }} | grep /etcd:"
  register: shell_etcd_image
  run_once: true
  delegate_to: "{{ selected_host }}"
  #when: healthy_node is defined

- name: kubernetes static manifests
  vars:
    etcd_image: "{{ shell_etcd_image.stdout }}"
  template:
    src: "{{ item }}"
    dest: "/etc/kubernetes/{{ item }}"
  loop:
  - manifests/etcd.yaml
  - manifests/kube-apiserver.yaml
  - manifests/kube-controller-manager.yaml
  - manifests/kube-scheduler.yaml

- name: enable kubelet service
  service:
    name: kubelet
    enabled: yes
    state: restarted

- block:
  - name: get token
    shell: kubeadm token create
    register: token

  - name: get certificate key
    shell: kubeadm init phase upload-certs --upload-certs | grep -v upload-certs
    register: cert_key

  # Get cert info
  - name: Get CA certificate hash
    community.crypto.x509_certificate_info:
      path: /etc/kubernetes/pki/ca.crt
    register: __k8s_pki_ca

  # Use the info
  - name: debug
    debug:
      msg: 'sha256:{{ __k8s_pki_ca["public_key_fingerprints"]["sha256"] | replace(":","") }}\ncert_key:{{cert_key}} token:{{token}}'

  - name: populate join vars to all hosts
    set_fact: 
      #join_command: "{{hostvars[healthy_node].join_command}}"
      cert_key: "{{hostvars[healthy_node].cert_key.stdout}}"
      token: "{{ hostvars[healthy_node].token.stdout }}"
      discovery_token_ca_cert_hash: 'sha256:{{ hostvars[healthy_node].__k8s_pki_ca["public_key_fingerprints"]["sha256"] | replace(":","") }}'
    delegate_to: "{{ item }}"
    delegate_facts: true
    loop: "{{ groups['all'] }}"
  run_once: true
  delegate_to: "{{ healthy_node }}"

- name: debug join command
  debug:
    msg: "cert_key:{{cert_key}} token:{{token}} discovery_token_ca_cert_hash:{{ discovery_token_ca_cert_hash | default('none') }}"
  
- block:
  - name: tempfile for kubeadm
    tempfile:
      state: file
      suffix: temp
    register: kubeadm_tmp
  - name: copy tempfile for kubeadm
    template:
      src: kubeadm.yaml
      dest: "{{ kubeadm_tmp.path }}"
    vars: 
      join_ip: "{{ my_cluster_ip }}"
      cert_key: cert_key.stdout

  - name: remove node object from kubernetes cluster
    run_once: true 
    delegate_to: "{{ healthy_node }}"
    shell:  (KUBECONFIG={{ kubeconfig }} kubectl delete node {{ inventory_hostname }} | true) 

  - name: remove node object from etcd cluster
    run_once: true
    delegate_to: "{{ healthy_node }}"
    shell: |
      params="-- etcdctl   --endpoints=https://127.0.0.1:2379   --cacert=/etc/kubernetes/pki/etcd/ca.crt   --cert=/etc/kubernetes/pki/etcd/server.crt   --key=/etc/kubernetes/pki/etcd/server.key"
      member_id=$(kubectl exec -n kube-system etcd-{{ healthy_node }} ${params} member list | grep ", {{ inventory_hostname }}, " | cut -d , -f 1)
      [ "${member_id}" != "" ] && kubectl exec -n kube-system etcd-{{ healthy_node }} ${params} member remove ${member_id}

  - name: joining node
    shell: rc-service kubelet stop; kubeadm reset --force && rm -f /etc/kubernetes/manifests/*.yaml /var/lib/kubelet/* /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /var/lib/etcd/* && kubeadm join "{{ controlPlaneEndpoint }}" --config "{{ kubeadm_tmp.path }}" --ignore-preflight-errors=DirAvailable--var-lib-etcd
    throttle: 1
  when: not k8s_installed

- block:
    - name: update k8s
      shell: |
        KUBECONFIG={{ kubeconfig }} kubeadm upgrade apply --config /etc/kubernetes/kubeadm.conf --yes "{{ k8s_version }}"
    - name: Wait for node to report Ready status
      shell: kubectl get node {{ inventory_hostname }} -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}'
      delegate_to: "{{ healthy_node  }}"
      register: node_status
      until: node_status.stdout == "True"
      retries: 30
      delay: 10
  throttle: 1
  when: k8s_upgrade or ('update' in ansible_run_tags)
  
- block:
  - name: taint to allow master nodes to run pods
    shell: kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule- || true

  - name: ca template
    template:
      src: ca
      dest: /tmp/myca

  - name: create CA configmap
    shell: |
      kubectl apply -f /tmp/myca

  - name: coredns configmap template
    template:
      src: coredns.cm.yaml
      dest: /tmp/coredns.cm.yaml

  - name: additional DNS entries and external CoreDNS IP
    shell: |
      kubectl replace -f /tmp/coredns.cm.yaml
      kubectl patch svc --namespace kube-system kube-dns --patch '{"spec": { "externalIPs": ["{{ k8s.external_coredns_ip }}"] }}'

  - name: ensure configmap kublet-config is refreshed from "{{ kubelet_config }}"
    shell: |
      [ -f "{{ kubelet_config }}" ] && kubectl create configmap kubelet-config --namespace kube-system --from-file=kubelet="{{ kubelet_config }}" --dry-run=client -o yaml | kubectl apply -f -

  - name: create storageClass local as default
    shell: |
      kubectl apply -f -<<EOF
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: local
      provisioner: kubernetes.io/no-provisioner
      reclaimPolicy: Retain
      volumeBindingMode: WaitForFirstConsumer
      EOF
      kubectl patch storageclass local -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
  when: inventory_hostname == healthy_node # to ensure this is executed on a healthy node only
     
