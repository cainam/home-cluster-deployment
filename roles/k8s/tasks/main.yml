---
- set_fact:
    selected_host: "{{ groups['all'] | map('extract', hostvars) | selectattr('system_roles', 'defined') | selectattr('system_roles', 'contains', 'control_plane') | map(attribute='inventory_hostname') | random }}"
    control_plane: "{{ groups['all'] | map('extract', hostvars) | selectattr('system_roles', 'defined') | selectattr('system_roles', 'contains', 'control_plane') | map(attribute='inventory_hostname') }}"
    worker: "{{ groups['all'] | map('extract', hostvars) | selectattr('system_roles', 'defined') | selectattr('system_roles', 'contains', 'worker') | map(attribute='inventory_hostname') }}"

- name: system configurations
  import_tasks: system.yml
  when: inventory_hostname in worker or inventory_hostname in control_plane

- name: force-reinstall
  shell: /etc/init.d/kubelet stop; kubeadm reset --force; rm -rf kubeadm.log /etc/kubernetes/pki/ /etc/kubernetes/manifests/ /etc/kubernetes/tmp/ /etc/kubernetes/*.conf  /var/log/kubelet/kubelet.log /var/log/crio/crio.log /var/lib/kubelet/*; iptables -F; iptables -t nat -F; /etc/init.d/crio restart; ip link delete dev cni0; ip link delete dev flannel.1; rm  /opt/cni/bin/flannel /etc/cni/net.d/10-flannel.conflist; /etc/init.d/kubelet restart
  ignore_errors: yes
  when: "'force_reinstall' in ansible_run_tags" 
  tags:
    - never
    - force_reinstall

- name: check kubernetes status
  import_tasks: k8s-status.yaml

- name: prepare images
  import_tasks: prepare_images.yml

- name: install cluster if needed
  include_tasks: init.yml
  when: not k8s_init_done and inventory_hostname == selected_host

- name: get kube-dns service IP
  shell: kubectl -n kube-system get svc kube-dns -o jsonpath='{.spec.clusterIP}'
  register: kube_dns_ip
  run_once: true
  delegate_to: "{{ healthy_node }}"

- name: get etcd image
  shell:  kubectl get pod -n kube-system etcd-{{ healthy_node }}  -o yaml -o jsonpath='{.spec.containers[?(@.name=="etcd")].image}'
  register: shell_etcd_image
  run_once: true
  delegate_to: "{{ healthy_node }}"

- name: set etcd image variable
  set_fact:
    etcd_image: "{{ shell_etcd_image.stdout }}"
  run_once: true

- name: kubelet configuration
  vars:
    dns_ip: "{{ kube_dns_ip.stdout }}"
  template:
    src: kubelet-config.yaml
    dest: "{{ kubelet_config }}"
  when: kube_dns_ip is defined and kube_dns_ip.rc == 0 

- name: enable kubelet service
  service:
    name: kubelet
    enabled: yes
    state: restarted

- name: kubernetes static manifests
  template:
    src: "{{ item }}"
    dest: "/etc/kubernetes/{{ item }}"
  loop:
  - manifests/etcd.yaml
  - manifests/kube-apiserver.yaml
  - manifests/kube-controller-manager.yaml
  - manifests/kube-scheduler.yaml

- block:
  - name: install flannel pod network
    shell: |
      mkdir -p /tmp/flannel; cd /tmp/flannel
      curl -O -L https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      sed -i -e '/k8s-app:/d' -e 's/- --kube-subnet-mgr/- --kube-subnet-mgr\n        - --iface=end0/'  kube-flannel.yml
      grep image kube-flannel.yml | awk '{print $2}' | sort -u | while read i; do echo "arm64 $i /k8s/ # required for kubernetes" | bash pull-tag-push.sh; done
      #echo -e 'apiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-target-namespace\n  labels:\n    pod-security.kubernetes.io/enforce: privileged' > namespace.yaml
      echo -e 'resources:\n- namespace.yaml\n- kube-flannel.yml' > kustomization.yaml
      kubectl apply -k .
    register: flannel_install

  - name: output flannel install
    debug:
      msg: "{{flannel_install}}"
  run_once: true
  delegate_to: "{{ healthy_node }}"

- block:
  - name: get token
    shell: kubeadm token create
    register: token

  - name: get certificate key
    shell: kubeadm init phase upload-certs --upload-certs | grep -v upload-certs
    register: cert_key

  # Get cert info
  - name: Get CA certificate hash
    community.crypto.x509_certificate_info:
      path: /etc/kubernetes/pki/ca.crt
    register: __k8s_pki_ca

  # Use the info
  - name: debug
    debug:
      msg: 'sha256:{{ __k8s_pki_ca["public_key_fingerprints"]["sha256"] | replace(":","") }}\ncert_key:{{cert_key}} token:{{token}}'

  - name: populate join vars to all hosts
    set_fact: 
      #join_command: "{{hostvars[healthy_node].join_command}}"
      cert_key: "{{hostvars[healthy_node].cert_key.stdout}}"
      token: "{{ hostvars[healthy_node].token.stdout }}"
      discovery_token_ca_cert_hash: 'sha256:{{ hostvars[healthy_node].__k8s_pki_ca["public_key_fingerprints"]["sha256"] | replace(":","") }}'
    delegate_to: "{{ item }}"
    delegate_facts: true
    loop: "{{ groups['all'] }}"
  when: inventory_hostname == healthy_node

- name: debug join command
  debug:
    msg: "cert_key:{{cert_key}} token:{{token}} discovery_token_ca_cert_hash:{{ discovery_token_ca_cert_hash | default('none') }}"
  
- block:
  - name: tempfile for kubeadm
    tempfile:
      state: file
      suffix: temp
    register: kubeadm_tmp
  - name: copy tempfile for kubeadm
    template:
      src: kubeadm.yaml
      dest: "{{ kubeadm_tmp.path }}"
    vars: 
      join_ip: "{{ my_cluster_ip }}"
      cert_key: cert_key.stdout

  - name: remove node object from cluster
    run_once: true 
    delegate_to: "{{ healthy_node }}"
    shell:  (KUBECONFIG={{ kubeconfig }} kubectl delete node {{ inventory_hostname }} | true)

  - name: joining node
    shell: kubeadm reset --force && rm -f /etc/kubernetes/manifests/*.yaml /var/lib/kubelet/* /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /var/lib/etcd/* && kubeadm join "{{ controlPlaneEndpoint }}" --config "{{ kubeadm_tmp.path }}" --ignore-preflight-errors=DirAvailable--var-lib-etcd
    throttle: 1
  when: not k8s_installed

- block:
    - name: update k8s
      shell: |
        KUBECONFIG={{ kubeconfig }} kubeadm upgrade apply --config /etc/kubernetes/kubeadm.conf --yes "{{ k8s_version }}"
    - name: Wait for node to report Ready status
      shell: kubectl get node {{ inventory_hostname }} -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}'
      delegate_to: "{{ healthy_node  }}"
      register: node_status
      until: node_status.stdout == "True"
      retries: 30
      delay: 10
  throttle: 1
  when: k8s_upgrade or ('update' in ansible_run_tags)
  
- block:
  - name: taint to allow master nodes to run pods
    shell: kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule- || true

  - name: ca template
    template:
      src: ca
      dest: /tmp/myca

  - name: create CA configmap
    shell: |
      kubectl apply -f /tmp/myca

  - name: coredns configmap template
    template:
      src: coredns.cm.yaml
      dest: /tmp/coredns.cm.yaml

  - name: additional DNS entries and external CoreDNS IP
    shell: |
      kubectl replace -f /tmp/coredns.cm.yaml
      kubectl patch svc --namespace kube-system kube-dns --patch '{"spec": { "externalIPs": ["{{ k8s.external_coredns_ip }}"] }}'

  - name: ensure configmap kublet-config is refreshed from "{{ kubelet_config }}"
    shell: |
      [ -f "{{ kubelet_config }}" ] && kubectl create configmap kubelet-config --namespace kube-system --from-file=kubelet="{{ kubelet_config }}" --dry-run=client -o yaml | kubectl apply -f -

  - name: create storageClass local as default
    shell: |
      kubectl apply -f -<<EOF
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: local
      provisioner: kubernetes.io/no-provisioner
      reclaimPolicy: Retain
      volumeBindingMode: WaitForFirstConsumer
      EOF
      kubectl patch storageclass local -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
  when: inventory_hostname == healthy_node # to ensure this is executed on a healthy node only
     
