---
- name: get seed
  shell: echo $RANDOM
  run_once: true
  register: seed
  
- set_fact:
    selected_host: "{{ ansible_play_hosts | random(seed=seed.stdout) }}"

- name: system configurations
  import_tasks: system.yml

- name: force-reinstall
  shell: /etc/init.d/kubelet stop; kubeadm reset --force; rm -rf kubeadm.log /etc/kubernetes/pki/ /etc/kubernetes/manifests/ /etc/kubernetes/tmp/ /etc/kubernetes/*.conf  /var/log/kubelet/kubelet.log /var/log/crio/crio.log /var/lib/kubelet/*; iptables -F; iptables -t nat -F; /etc/init.d/crio restart; ip link delete dev cni0; ip link delete dev flannel.1; rm  /opt/cni/bin/flannel /etc/cni/net.d/10-flannel.conflist; /etc/init.d/kubelet restart
  ignore_errors: yes
  when: "'force_reinstall' in ansible_run_tags" 
  tags:
    - never
    - force_reinstall

- name: check kubernetes status
  import_tasks: k8s-status.yaml

- name: prepare images
  import_tasks: prepare_images.yml
  run_once: true

- name: install cluster if needed
  include_tasks: init.yml
  when: not k8s_init_done and inventory_hostname == selected_host

- name: get kube-dns service IP
  shell: kubectl -n kube-system get svc kube-dns -o jsonpath='{.spec.clusterIP}'
  register: kube_dns_ip
  run_once: true
  delegate_to: "{{ healthy_node }}"

- name: get etcd image
  shell:  kubectl get pod -n kube-system etcd-{{ healthy_node }}  -o yaml -o jsonpath='{.spec.containers[?(@.name=="etcd")].image}'
  register: shell_etcd_image
  run_once: true
  delegate_to: "{{ healthy_node }}"

- name: set etcd image variable
  set_fact:
    etcd_image: "{{ shell_etcd_image.stdout }}"
  run_once: true

- name: kubelet configuration
  vars:
    dns_ip: "{{ kube_dns_ip.stdout }}"
  template:
    src: kubelet-config.yaml
    dest: /var/lib/kubelet/config.yaml
  when: kube_dns_ip is defined and kube_dns_ip.rc == 0

- name: enable kubelet service
  service:
    name: kubelet
    enabled: yes
    state: restarted

- name: kubernetes static manifests
  template:
    src: "{{ item }}"
    dest: "/etc/kubernetes/{{ item }}"
  loop:
  - manifests/etcd.yaml
  - manifests/kube-apiserver.yaml
  - manifests/kube-controller-manager.yaml
  - manifests/kube-scheduler.yaml

- block:
  - name: build flannel
    shell: |
      . set_env.sh kube-flannel
      helm:from_git_to_local.sh --platform="{{ item.platform | default(default_platform) }}" --git_source=https://github.com/flannel-io/flannel.git
      helm repo add kube-flannel "{{ helm_repo_base }}kube-flannel"
      helm repo index "${helm_repo_dir}" --url "{{ helm_repo_base }}kube-flannel"
    
  - name: install flannel pod network
    shell: |
      . set_env.sh kube-flannel
      kubectl get ns kube-flannel 2>/dev/null > /dev/null || kubectl create ns kube-flannel
      kubectl label --overwrite ns kube-flannel pod-security.kubernetes.io/enforce=privileged
      helm upgrade --reset-values --install -n kube-flannel kube-flannel kube-flannel/flannel --set 'flannel.args={--iface=end0,--ip-masq,--kube-subnet-mgr}'  
    register: flannel_install

  - name: output flannel install
    debug:
      msg: "{{flannel_install}}"
  when: inventory_hostname == healthy_node and (not k8s_init_done or k8s_upgrade)
  
- block:
  - name: get token
    shell: kubeadm token create
    register: token

  - name: get certificate key
    shell: kubeadm init phase upload-certs --upload-certs | grep -v upload-certs
    register: cert_key

  # Get cert info
  - name: Get CA certificate hash
    community.crypto.x509_certificate_info:
      path: /etc/kubernetes/pki/ca.crt
    register: __k8s_pki_ca

  # Use the info
  - name: debug
    debug:
      msg: 'sha256:{{ __k8s_pki_ca["public_key_fingerprints"]["sha256"] | replace(":","") }}\ncert_key:{{cert_key}} token:{{token}}'

  - name: populate join vars to all hosts
    set_fact: 
      #join_command: "{{hostvars[healthy_node].join_command}}"
      cert_key: "{{hostvars[healthy_node].cert_key.stdout}}"
      token: "{{ hostvars[healthy_node].token.stdout }}"
      discovery_token_ca_cert_hash: 'sha256:{{ hostvars[healthy_node].__k8s_pki_ca["public_key_fingerprints"]["sha256"] | replace(":","") }}'
    delegate_to: "{{ item }}"
    delegate_facts: true
    loop: "{{ groups['all'] }}"
  when: inventory_hostname == healthy_node

- name: debug join command
  debug:
    msg: "cert_key:{{cert_key}} token:{{token}} discovery_token_ca_cert_hash:{{ discovery_token_ca_cert_hash | default('none') }}"
  
- block:
  - name: tempfile for kubeadm
    tempfile:
      state: file
      suffix: temp
    register: kubeadm_tmp
  - name: copy tempfile for kubeadm
    template:
      src: kubeadm.yaml
      dest: "{{ kubeadm_tmp.path }}"
    vars: 
      join_ip: "{{ my_cluster_ip }}"
      cert_key: cert_key.stdout
  - name: joining nodes
    shell: kubeadm reset --force && rm -rf /etc/kubernetes/ && kubeadm join "{{ controlPlaneEndpoint }}" --config "{{ kubeadm_tmp.path }}" --ignore-preflight-errors=DirAvailable--var-lib-etcd
    throttle: 1
  when: k8s_healthy and not k8s_installed

# update if version is newer   
- name: update k8s
  shell: |
    KUBECONFIG={{ kubeconfig }} kubeadm upgrade apply  --config /etc/kubernetes/kubeadm.conf --yes "{{ k8s_version }}"
  throttle: 1
  when: k8s_upgrade or ('update' in ansible_run_tags)
  
- block:
  - name: taint to allow master nodes to run pods
    shell: |
      # kubectl get node -o custom-columns=NAME:.metadata.name,TAINT:.spec.taints[*].key,TAINT:.spec.taints[*].effect | grep -e master:NoSchedule >/dev/null && kubectl taint nodes --all node-role.kubernetes.io/master:NoSchedule-
      kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule-
      true
  - name: ca template
    template:
      src: ca
      dest: /tmp/myca

  - name: create CA configmap
    shell: |
      kubectl apply -f /tmp/myca

  - name: coredns configmap template
    template:
      src: coredns.cm.yaml
      dest: /tmp/coredns.cm.yaml

  - name: additional DNS entries and external CoreDNS IP
    shell: |
      kubectl replace -f /tmp/coredns.cm.yaml
      kubectl patch svc --namespace kube-system kube-dns --patch '{"spec": { "externalIPs": ["{{ k8s.external_coredns_ip }}"] }}'

  - name: ensure /var/lib/kubelet/config.yaml is refreshed
    shell: |
      [ -f /var/lib/kubelet/config.yaml ] && cp -dp /var/lib/kubelet/config.yaml /var/lib/kubelet/config.yaml.bck
      kubectl get configmaps -n kube-system kubelet-config -o jsonpath='{.data.kubelet}' > /var/lib/kubelet/config.yaml

  - name: create storageClass local as default
    shell: |
      kubectl apply -f -<<EOF
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: local
      provisioner: kubernetes.io/no-provisioner
      reclaimPolicy: Retain
      volumeBindingMode: WaitForFirstConsumer
      EOF
      kubectl patch storageclass local -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
  when: inventory_hostname == healthy_node # to ensure this is executed on a healthy node only
     
