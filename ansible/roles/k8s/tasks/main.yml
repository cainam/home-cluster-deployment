---
- name: get seed
  shell: echo $RANDOM
  run_once: true
  register: seed
  
- set_fact:
    selected_host: "{{ ansible_play_hosts | random(seed=seed.stdout) }}"

- debug: 
    msg: "selected_host:{{ selected_host }}"

- name: system configurations
  import_tasks: system.yml

- name: force-reinstall
  shell: /etc/init.d/kubelet stop; kubeadm reset --force; rm -rf kubeadm.log /etc/kubernetes/pki/ /etc/kubernetes/manifests/ /etc/kubernetes/tmp/ /etc/kubernetes/*.conf  /var/log/kubelet/kubelet.log /var/log/crio/crio.log /var/lib/kubelet/*; iptables -F; iptables -t nat -F; /etc/init.d/crio restart; ip link delete dev cni0; ip link delete dev flannel.1; rm  /opt/cni/bin/flannel /etc/cni/net.d/10-flannel.conflist; /etc/init.d/kubelet restart
  ignore_errors: yes
  when: "'force_reinstall' in ansible_run_tags" 
  tags:
    - never
    - force_reinstall

- name: check kubernetes status
  import_tasks: k8s-status.yaml

- name: prepare images
  import_tasks: prepare_images.yml
  delegate_to: "{{ selected_host }}"
  run_once: true

- name: handle additional images
  vars:
    limit_images:
    - helm
    - registry
  include_role:
    name: gentoo-image-builder
  run_once: true

- name: install cluster if needed
  include_tasks: init.yml
  when: not k8s_init_done and inventory_hostname == selected_host

- name: install flannel pod network
  shell: |
    . set_env.sh kube-flannel
    kubectl get ns kube-flannel 2>/dev/null > /dev/null || kubectl create ns kube-flannel
    kubectl label --overwrite ns kube-flannel pod-security.kubernetes.io/enforce=privileged
    helm upgrade --reset-values --install -n kube-flannel kube-flannel kube-flannel/flannel --set 'flannel.args={--iface=end0,--ip-masq,--kube-subnet-mgr}'  
  register: flannel_install
  when: inventory_hostname == healthy_node
  
- name: output flannel install
  debug:
      msg: "{{flannel_install}}"

- block:
  - name: get token
    #shell: kubeadm token create --print-join-command
    shell: kubeadm token create
    register: token
    #when: k8s_healthy and k8s_installed

  - name: get certificate key
    shell: kubeadm init phase upload-certs --upload-certs | grep -v upload-certs
    register: cert_key
    #when: k8s_healthy and k8s_installed

  # Get cert info
  - name: Get CA certificate hash
    community.crypto.x509_certificate_info:
      path: /etc/kubernetes/pki/ca.crt
    register: __k8s_pki_ca

  # Use the info
  - name: debug
    debug:
      msg: 'sha256:{{ __k8s_pki_ca["public_key_fingerprints"]["sha256"] | replace(":","") }}\ncert_key:{{cert_key}} token:{{token}}'

  - name: populate join vars to all hosts
    set_fact: 
      #join_command: "{{hostvars[healthy_node].join_command}}"
      cert_key: "{{hostvars[healthy_node].cert_key.stdout}}"
      token: "{{ hostvars[healthy_node].token.stdout }}"
      discovery_token_ca_cert_hash: 'sha256:{{ hostvars[healthy_node].__k8s_pki_ca["public_key_fingerprints"]["sha256"] | replace(":","") }}'
    delegate_to: "{{ item }}"
    delegate_facts: true
    loop: "{{ groups['all'] }}"
  when: inventory_hostname == healthy_node

- name: debug join command
  debug:
    msg: "cert_key:{{cert_key}} token:{{token}} discovery_token_ca_cert_hash:{{ discovery_token_ca_cert_hash | default('none') }}"
  
- block:
  - name: tempfile for kubeadm
    tempfile:
      state: file
      suffix: temp
    register: kubeadm_tmp
  - name: copy tempfile for kubeadm
    template:
      src: kubeadm.yaml
      dest: "{{ kubeadm_tmp.path }}"
    vars: 
      join_ip: "{{ my_cluster_ip }}"
      cert_key: cert_key.stdout
  - name: joining nodes
#    shell: '{{ join_command.stdout }} --control-plane --certificate-key {{ cert_key.stdout }} --config "{{ kubeadm_tmp.path }}"'
    shell: kubeadm reset && rm -rf /etc/kubernetes/ && kubeadm join "{{ controlPlaneEndpoint }}" --config "{{ kubeadm_tmp.path }}"
    throttle: 1
  when: k8s_healthy and not k8s_installed

# update if version is newer   
- name: update k8s
  shell: |
    KUBECONFIG=/etc/kubernetes/admin.conf kubeadm upgrade apply  --config /etc/kubernetes/kubeadm.conf --yes "{{ k8s_version }}"
  throttle: 1
  when: k8s_upgrade or ('update' in ansible_run_tags)
  
- block:
  - name: ensure kube-apiserver has audit-log with its volumes enabled
    shell: |
      yq -yi '
      .spec.volumes = ((.spec.volumes // []) | map(select(.name != "AdmissionConfiguration" and .name != "audit" and .name != "audit-log"))+
      [{name: "audit","hostPath": {type: "File",path: "/etc/kubernetes/audit-policy.yaml"}},
       {name: "audit-log","hostPath": {type: "FileOrCreate",path: "/var/log/kubernetes/audit/audit.log"}},
       {name: "AdmissionConfiguration","hostPath": {type: "FileOrCreate",path: "/etc/kubernetes/AdmissionConfiguration.yaml"}}
       ]) |
      .spec.containers[] |= (
       if .name == "kube-apiserver" then
         .command = ( (.command // []) # remove parameters if existing and add new ones
              | map(select(startswith("--enable-admission-plugins=") | not))
              + ["--enable-admission-plugins=NodeRestriction,PodSecurity"]
              | map(select(startswith("--admission-control-config-file=") | not))
              + ["--admission-control-config-file=/etc/kubernetes/AdmissionConfiguration.yaml"]
              | map(select(startswith("--audit-log-path=") | not))
              + ["--audit-log-path=/var/log/kubernetes/audit/audit.log"]
              | map(select(startswith("--audit-policy-file=") | not))
              + ["--audit-policy-file=/etc/kubernetes/audit-policy.yaml"]
         )|
         .volumeMounts = ( (.volumeMounts // []) | map(select(.name != "AdmissionConfiguration" and .name != "audit" and .name != "audit-log"))+
           [{name: "audit",mountPath: "/etc/kubernetes/audit-policy.yaml", readOnly: true},
            {name: "audit-log",mountPath: "/var/log/kubernetes/audit/audit.log", readOnly: false},
            {name: "AdmissionConfiguration",mountPath: "/etc/kubernetes/AdmissionConfiguration.yaml", readOnly: true}
           ])
       else . end )' /etc/kubernetes/manifests/kube-apiserver.yaml
      
- block:
  - name: taint to allow master nodes to run pods
    shell: |
      # kubectl get node -o custom-columns=NAME:.metadata.name,TAINT:.spec.taints[*].key,TAINT:.spec.taints[*].effect | grep -e master:NoSchedule >/dev/null && kubectl taint nodes --all node-role.kubernetes.io/master:NoSchedule-
      kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule-
      true
  - name: ca template
    template:
      src: ca
      dest: /tmp/myca

  - name: create CA configmap
    shell: |
      kubectl apply -f /tmp/myca

  - name: coredns configmap template
    template:
      src: coredns.cm.yaml
      dest: /tmp/coredns.cm.yaml
    #with_items: "{{ gateways }}"

  - name: additional DNS entries and external CoreDNS IP
    shell: |
      kubectl replace -f /tmp/coredns.cm.yaml
      kubectl patch svc --namespace kube-system kube-dns --patch '{"spec": { "externalIPs": ["{{ external_coredns_ip }}"] }}'

  - name: create storageClass local as default
    shell: |
      kubectl apply -f -<<EOF
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: local
      provisioner: kubernetes.io/no-provisioner
      reclaimPolicy: Retain
      volumeBindingMode: WaitForFirstConsumer
      EOF
      kubectl patch storageclass local -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
  when: inventory_hostname == healthy_node # to ensure this is executed on a healthy node only
     
